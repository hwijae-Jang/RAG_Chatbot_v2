%%writefile app_before.py

import os

import streamlit as st
from pathlib import Path

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="Before - í•­ê³µê¶Œ í™˜ë¶ˆ ì±—ë´‡", layout="wide")
st.title("âœˆï¸ Before ë²„ì „ - í™˜ë¶ˆ ìƒë‹´ ì±—ë´‡")
st.markdown("### ğŸ”´ ë¬¸ì œ ìƒí™© ì¬í˜„ ë²„ì „")
st.caption("chunk_size=800, overlap=100, ë™ì˜ì–´ ì—†ìŒ")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“š Before ì„¤ì •")
    st.info("âš ï¸ ì´ ë²„ì „ì€ ì´ˆê¸° ìƒíƒœë¥¼ ì¬í˜„í•©ë‹ˆë‹¤")

    # API í‚¤ ì…ë ¥
    api_key = st.text_input("OpenAI API Key", type="password", value=os.getenv("OPENAI_API_KEY", ""))
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key

    k = st.slider("ê²€ìƒ‰ ê°œìˆ˜ k", 1, 5, 3)
    show_sources = st.checkbox("ê·¼ê±° í‘œì‹œ", value=True)

    st.divider()

    st.header("â“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸")
    test_questions = [
        "ì œì£¼í•­ê³µ ë…¸ì‡¼ ìœ„ì•½ê¸ˆì€?",
        "ì§„ì—ì–´ êµ­ë‚´ì„  ë…¸ì‡¼ ìœ„ì•½ê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "no-show penalty",
        "ëŒ€í•œí•­ê³µ êµ­ì œì„  í™˜ë¶ˆ ìˆ˜ìˆ˜ë£Œ í‘œ",
        "ì œì£¼í•­ê³µ FLEX ìš´ì„ í™˜ë¶ˆ"
    ]

    for q in test_questions:
        if st.button(q, key=q):
            st.session_state["question"] = q

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.pop("messages", None)
        st.rerun()

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "question" not in st.session_state:
    st.session_state["question"] = None

# API í‚¤ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    st.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•˜ì„¸ìš”")
    st.stop()

# LLM ì´ˆê¸°í™”
@st.cache_resource
def init_llm():
    return ChatOpenAI(
        model='gpt-4o-mini',
        temperature=0,
        api_key=os.environ.get("OPENAI_API_KEY")
    )

llm = init_llm()

# í”„ë¡¬í”„íŠ¸
rag_prompt = ChatPromptTemplate.from_template("""
ë„ˆëŠ” í•­ê³µê¶Œ í™˜ë¶ˆ ë° ë³€ê²½ì„ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ ìƒë‹´ ì±—ë´‡ì´ì•¼.
ì•„ë˜ í•­ê³µì‚¬ ì •ì±… ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜.

ì •ì±…ë¬¸ì„œ: {context}
ì‚¬ìš©ì ì§ˆë¬¸: {q}

ë‹µë³€ ì‘ì„± ì‹œ ìœ ì˜ì‚¬í•­:
1. ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì¤˜
2. í™˜ë¶ˆ/ë³€ê²½ ìˆ˜ìˆ˜ë£Œ, ê¸°ê°„ ë“± êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ëª…í™•íˆ ì œì‹œí•´ì¤˜
3. ë‹µë³€ ë§ˆì§€ë§‰ì— "âš ï¸ ì •í™•í•œ ì •ë³´ëŠ” í•­ê³µì‚¬ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”"ë¼ê³  ì•ˆë‚´í•´ì¤˜

ë‹µë³€:
""")

rag_chain = rag_prompt | llm | StrOutputParser()

# ë²¡í„° DB ì´ˆê¸°í™”
@st.cache_resource
def initialize_vectordb():
    """Before ì„¤ì •ìœ¼ë¡œ ë²¡í„° DB ì´ˆê¸°í™”"""

    # MD íŒŒì¼ ê²½ë¡œ (Corrected path)
    md_path = Path("/content/before")

    # MD íŒŒì¼ ë¡œë“œ
    docs = []
    for md_file in md_path.glob("*.md"):
        try:
            loader = TextLoader(str(md_file), encoding='utf-8')
            file_docs = loader.load()

            for doc in file_docs:
                doc.metadata['source'] = md_file.name

            docs.extend(file_docs)
        except Exception as e:
            st.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {md_file.name}")
            continue

    if not docs:
        st.error("MD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        st.stop()

    # Before ì„¤ì •ìœ¼ë¡œ ì²­í‚¹
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,  # Before: ì‘ì€ ì²­í¬
        chunk_overlap=100
    )
    chunks = splitter.split_documents(docs)

    # ë²¡í„° DB ìƒì„±
    embeddings = OpenAIEmbeddings(
        model='text-embedding-3-small',
        api_key=os.environ.get("OPENAI_API_KEY")
    )

    db = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name="before_chatbot"
    )

    return db, len(docs), len(chunks)

with st.spinner("ğŸ”„ ë²¡í„° DB ì´ˆê¸°í™” ì¤‘..."):
    db, num_docs, num_chunks = initialize_vectordb()

st.sidebar.success(f"âœ… {num_docs}ê°œ ë¬¸ì„œ, {num_chunks}ê°œ ì²­í¬")

# ë¼ìš°íŒ… í‚¤ì›Œë“œ (Before ë²„ì „ - ë™ì˜ì–´ ì—†ìŒ)
RAG_KEYWORDS = [
    "í™˜ë¶ˆ", "êµí™˜", "ë°˜í’ˆ", "ì •ì±…", "í•­ê³µê¶Œ", "ë³€ê²½", "ì·¨ì†Œ", "ìš´ì„",
    "FLEX", "SAVE", "êµ­ë‚´ì„ ", "êµ­ì œì„ ", "ìˆ˜ìˆ˜ë£Œ", "ê¸°ê°„", "No-Show",
    "ë…¸ì‡¼", "íŒ¨í‚¤ì§€", "ì²˜ë¦¬", "ìœ íš¨ê¸°ê°„"
]

def route_to_rag(q):
    """ë¼ìš°íŒ… íŒë‹¨"""
    ql = q.lower()
    return any(kw.lower() in ql for kw in RAG_KEYWORDS)

def refund_rag(question, k_val):
    """RAG ê²€ìƒ‰ ë° ë‹µë³€"""
    results = db.similarity_search_with_relevance_scores(question, k=k_val)

    if not results:
        return "âŒ ê´€ë ¨ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", []

    context = "\n\n".join([doc.page_content for doc, score in results])
    answer = rag_chain.invoke({'context': context, 'q': question})

    sources = []
    for doc, score in results:
        sources.append({
            'filename': doc.metadata.get('source', 'unknown'),
            'score': score,
            'content': doc.page_content[:150]
        })

    return answer, sources

# ì±„íŒ… UI
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì…ë ¥ ì²˜ë¦¬
if st.session_state["question"]:
    question = st.session_state["question"]
    st.session_state["question"] = None
else:
    question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if question:
    # ì‚¬ìš©ì ë©”ì‹œì§€
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state["messages"].append({"role": "user", "content": question})

    # ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€
    with st.chat_message("assistant"):
        if route_to_rag(question):
            with st.spinner("ğŸ” ê²€ìƒ‰ ì¤‘..."):
                answer, sources = refund_rag(question, k)

            st.markdown(answer)

            # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
            if sources:
                st.success(f"âœ… {len(sources)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            else:
                st.error("âŒ ê²€ìƒ‰ ì‹¤íŒ¨")

            if show_sources and sources:
                with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ"):
                    for i, src in enumerate(sources, 1):
                        st.markdown(f"**{i}. {src['filename']}** (ìœ ì‚¬ë„: {src['score']:.2f})")
                        st.caption(src['content'][:100] + "...")
                        st.divider()
        else:
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤. í•­ê³µê¶Œ í™˜ë¶ˆ/ë³€ê²½ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
            st.markdown(answer)

    st.session_state["messages"].append({"role": "assistant", "content": answer})

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
st.caption("ğŸ”´ Before ë²„ì „ - ë¬¸ì œ ìƒí™© ì¬í˜„")
st.caption("íŠ¹ì§•: chunk_size=800, ë™ì˜ì–´ ì‚¬ì „ ì—†ìŒ, ê¸°ë³¸ RAG")
