
"""
After ë²„ì „ í•­ê³µê¶Œ í™˜ë¶ˆ ìƒë‹´ RAG ì±—ë´‡
- chunk_size: 2000
- chunk_overlap: 400
- ë™ì˜ì–´ ì‚¬ì „: 50+
- ëŒ€í•œí•­ê³µ í†µí•© (6ê°œ md íŒŒì¼)
"""

import os
import streamlit as st
from pathlib import Path

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="After - í•­ê³µê¶Œ í™˜ë¶ˆ ì±—ë´‡", layout="wide")
st.title("âœˆï¸ After ë²„ì „ - í™˜ë¶ˆ ìƒë‹´ ì±—ë´‡")
st.markdown("### ğŸŸ¢ ìµœì¢… ê°œì„  ë²„ì „")
st.caption("chunk_size=2000, overlap=400, ë™ì˜ì–´ 50+")

# ë™ì˜ì–´ ì‚¬ì „ (After í•µì‹¬ ê°œì„ )
SYNONYM_DICT = {
    # ë…¸ì‡¼ ê´€ë ¨
    "ë…¸ì‡¼": ["ë…¸ì‡¼", "No-Show", "no-show", "ë…¸ ì‡¼", "ë¯¸íƒ‘ìŠ¹", "ì˜ˆì•½ë¶€ë„"],
    "no-show": ["ë…¸ì‡¼", "No-Show", "no-show", "ë¯¸íƒ‘ìŠ¹", "ì˜ˆì•½ë¶€ë„"],
    "ì˜ˆì•½ë¶€ë„": ["ë…¸ì‡¼", "ì˜ˆì•½ë¶€ë„", "no-show", "ë¯¸íƒ‘ìŠ¹"],

    # í™˜ë¶ˆ ê´€ë ¨
    "í™˜ë¶ˆ": ["í™˜ë¶ˆ", "refund", "ë°˜í™˜", "ì·¨ì†Œí™˜ë¶ˆ"],
    "refund": ["í™˜ë¶ˆ", "refund", "ë°˜í™˜"],

    # ë³€ê²½ ê´€ë ¨
    "ë³€ê²½": ["ë³€ê²½", "change", "ìˆ˜ì •", "êµí™˜"],
    "change": ["ë³€ê²½", "change", "ìˆ˜ì •"],

    # ìˆ˜ìˆ˜ë£Œ ê´€ë ¨
    "ìˆ˜ìˆ˜ë£Œ": ["ìˆ˜ìˆ˜ë£Œ", "fee", "ìš”ê¸ˆ", "ë¹„ìš©", "charge", "ìœ„ì•½ê¸ˆ", "íŒ¨ë„í‹°", "penalty"],
    "fee": ["ìˆ˜ìˆ˜ë£Œ", "fee", "ìš”ê¸ˆ", "ë¹„ìš©", "charge", "ìœ„ì•½ê¸ˆ", "íŒ¨ë„í‹°", "penalty"],
    "ìœ„ì•½ê¸ˆ": ["ìœ„ì•½ê¸ˆ", "íŒ¨ë„í‹°", "penalty", "ìˆ˜ìˆ˜ë£Œ", "fee"],
    "penalty": ["ìœ„ì•½ê¸ˆ", "íŒ¨ë„í‹°", "penalty", "ìˆ˜ìˆ˜ë£Œ"],

    # ì·¨ì†Œ ê´€ë ¨
    "ì·¨ì†Œ": ["ì·¨ì†Œ", "cancel", "cancellation", "í•´ì§€"],
    "cancel": ["ì·¨ì†Œ", "cancel", "cancellation"],

    # ìš´ì„ ì¢…ë¥˜
    "íŠ¹ê°€": ["íŠ¹ê°€", "íŠ¹ê°€ìš´ì„", "í”„ë¡œëª¨ì…˜", "promotion", "special"],
    "í• ì¸": ["í• ì¸", "í• ì¸ìš´ì„", "discount", "ì„¸ì¼", "sale"],
    "ì¼ë°˜": ["ì¼ë°˜", "ì¼ë°˜ìš´ì„", "ì •ìƒ", "ì •ìƒìš´ì„", "normal", "regular"],

    # ìš´ì„ ë“±ê¸‰
    "ë² ì´ì§": ["ë² ì´ì§", "BASIC", "Basic", "basic"],
    "basic": ["ë² ì´ì§", "BASIC", "Basic"],
    "ìŠ¤íƒ ë‹¤ë“œ": ["ìŠ¤íƒ ë‹¤ë“œ", "STANDARD", "Standard", "standard"],
    "standard": ["ìŠ¤íƒ ë‹¤ë“œ", "STANDARD", "Standard"],
    "í”Œë ‰ìŠ¤": ["í”Œë ‰ìŠ¤", "FLEX", "Flex", "flex", "flexible"],
    "flex": ["í”Œë ‰ìŠ¤", "FLEX", "Flex", "flexible"],
    "ì„¸ì´ë²„": ["ì„¸ì´ë²„", "SAVER", "Saver", "saver"],
    "saver": ["ì„¸ì´ë²„", "SAVER", "Saver"],

    # ë…¸ì„  ê´€ë ¨
    "êµ­ë‚´ì„ ": ["êµ­ë‚´ì„ ", "domestic", "êµ­ë‚´"],
    "domestic": ["êµ­ë‚´ì„ ", "domestic"],
    "êµ­ì œì„ ": ["êµ­ì œì„ ", "international", "êµ­ì œ", "í•´ì™¸"],
    "international": ["êµ­ì œì„ ", "international"],

    # íƒ‘ìŠ¹ìˆ˜ì† ê´€ë ¨
    "íƒ‘ìŠ¹ìˆ˜ì†": ["íƒ‘ìŠ¹ìˆ˜ì†", "ì²´í¬ì¸", "check-in", "ìˆ˜ì†"],
    "ì²´í¬ì¸": ["íƒ‘ìŠ¹ìˆ˜ì†", "ì²´í¬ì¸", "check-in"],
    "check-in": ["íƒ‘ìŠ¹ìˆ˜ì†", "ì²´í¬ì¸", "check-in"],

    # ê²Œì´íŠ¸ ê´€ë ¨
    "ê²Œì´íŠ¸": ["ê²Œì´íŠ¸", "gate", "ì¶œêµ¬ì¥"],
    "gate": ["ê²Œì´íŠ¸", "gate"],

    # ë¯¸íƒ‘ìŠ¹
    "ë¯¸íƒ‘ìŠ¹": ["ë¯¸íƒ‘ìŠ¹", "no-show", "ë¯¸ìŠ¹ì„ ", "ë¶ˆíƒ‘ìŠ¹"]
}

def expand_query_with_synonyms(query: str) -> str:
    """ë™ì˜ì–´ë¥¼ í™œìš©í•œ ì¿¼ë¦¬ í™•ì¥"""
    words = query.split()
    expanded_terms = []

    for word in words:
        word_lower = word.lower()
        if word_lower in SYNONYM_DICT:
            # ë™ì˜ì–´ ì¶”ê°€
            synonyms = SYNONYM_DICT[word_lower]
            expanded_terms.extend(synonyms[:3])  # ìµœëŒ€ 3ê°œ
        expanded_terms.append(word)

    return " ".join(expanded_terms)

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ“š After ì„¤ì •")
    st.success("âœ… ìµœì¢… ê°œì„  ë²„ì „")

    # API í‚¤ ì…ë ¥
    api_key = st.text_input("OpenAI API Key", type="password", value=os.getenv("OPENAI_API_KEY", ""))
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key

    k = st.slider("ê²€ìƒ‰ ê°œìˆ˜ k", 1, 5, 3)
    show_sources = st.checkbox("ê·¼ê±° í‘œì‹œ", value=True)
    use_synonyms = st.checkbox("ë™ì˜ì–´ í™•ì¥", value=True)

    st.divider()

    st.header("â“ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸")
    test_questions = [
        "ì œì£¼í•­ê³µ ë…¸ì‡¼ ìœ„ì•½ê¸ˆì€?",
        "ì§„ì—ì–´ êµ­ë‚´ì„  ë…¸ì‡¼ ìœ„ì•½ê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "no-show penalty",
        "ëŒ€í•œí•­ê³µ êµ­ì œì„  í™˜ë¶ˆ ìˆ˜ìˆ˜ë£Œ í‘œ",
        "ì œì£¼í•­ê³µ FLEX ìš´ì„ í™˜ë¶ˆ"
    ]

    for q in test_questions:
        if st.button(q, key=q):
            st.session_state["question"] = q

    st.divider()
    if st.button("ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.pop("messages", None)
        st.rerun()

    # ë™ì˜ì–´ ì •ë³´ í‘œì‹œ
    st.divider()
    with st.expander("ğŸ” ë™ì˜ì–´ ì‚¬ì „ ì •ë³´"):
        st.caption(f"ì´ {len(SYNONYM_DICT)}ê°œ í‚¤ì›Œë“œ")
        st.caption("ì˜ˆ: ë…¸ì‡¼ â†’ no-show, ì˜ˆì•½ë¶€ë„")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "question" not in st.session_state:
    st.session_state["question"] = None

# API í‚¤ í™•ì¸
if not os.getenv("OPENAI_API_KEY"):
    st.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥í•˜ì„¸ìš”")
    st.stop()

# LLM ì´ˆê¸°í™”
@st.cache_resource
def init_llm():
    return ChatOpenAI(
        model='gpt-4o-mini',
        temperature=0,
        api_key=os.environ.get("OPENAI_API_KEY")
    )

llm = init_llm()

# í”„ë¡¬í”„íŠ¸ (ë” ìƒì„¸í•˜ê²Œ ê°œì„ )
rag_prompt = ChatPromptTemplate.from_template("""
ë„ˆëŠ” í•­ê³µê¶Œ í™˜ë¶ˆ ë° ë³€ê²½ì„ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ ìƒë‹´ ì±—ë´‡ì´ì•¼.
ì•„ë˜ í•­ê³µì‚¬ ì •ì±… ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜.

ì •ì±…ë¬¸ì„œ: {context}
ì‚¬ìš©ì ì§ˆë¬¸: {q}

ë‹µë³€ ì‘ì„± ì‹œ ìœ ì˜ì‚¬í•­:
1. ë¬¸ì„œì— ìˆëŠ” ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì¤˜
2. í™˜ë¶ˆ/ë³€ê²½ ìˆ˜ìˆ˜ë£Œ, ê¸°ê°„ ë“± êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ëª…í™•íˆ ì œì‹œí•´ì¤˜
3. í‘œ(í…Œì´ë¸”) í˜•ì‹ì˜ ì •ë³´ê°€ ìˆë‹¤ë©´ **ì „ì²´ ë‚´ìš©**ì„ ë¹ ì§ì—†ì´ í¬í•¨í•´ì¤˜
4. ìš´ì„ ë“±ê¸‰ë³„ ì°¨ì´ê°€ ìˆë‹¤ë©´ ëª…í™•íˆ êµ¬ë¶„í•´ì„œ ì„¤ëª…í•´ì¤˜
5. ë‹µë³€ ë§ˆì§€ë§‰ì— "âš ï¸ ì •í™•í•œ ì •ë³´ëŠ” í•­ê³µì‚¬ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”"ë¼ê³  ì•ˆë‚´í•´ì¤˜

ë‹µë³€:
""")

rag_chain = rag_prompt | llm | StrOutputParser()

# ë²¡í„° DB ì´ˆê¸°í™”
@st.cache_resource
def initialize_vectordb():
    """After ì„¤ì •ìœ¼ë¡œ ë²¡í„° DB ì´ˆê¸°í™”"""

    # MD íŒŒì¼ ê²½ë¡œ
    # MD íŒŒì¼ ê²½ë¡œ íŒ¨í„´ (Streamlit Cloud ë°°í¬ ìµœì í™”)
    import glob
    patterns = [
        "data/after/*.md",       # Streamlit Cloud (ìƒëŒ€ ê²½ë¡œ)
        "./data/after/*.md",     # ë¡œì»¬ ì‹¤í–‰
        "../data/after/*.md",    # app/ í´ë”ì—ì„œ ì‹¤í–‰ ì‹œ
    ]

    # íŒŒì¼ ê²€ìƒ‰
    seen = set()
    md_files = []

    for pat in patterns:
        for fp in glob.glob(pat, recursive=True):
            if fp.endswith(".md") and fp not in seen and Path(fp).is_file():
                seen.add(fp)
                md_files.append(fp)

    if not md_files:
        st.error("âŒ MD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        st.info("""
        ## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
        
        **í•„ìš”í•œ êµ¬ì¡°:**
        ```
        RAG_Chatbot_v2/
        â”œâ”€â”€ app/
        â”‚   â””â”€â”€ app_after_deploy.py  â† ì´ íŒŒì¼
        â””â”€â”€ data/
            â””â”€â”€ after/               â† MD íŒŒì¼ ìœ„ì¹˜
                â”œâ”€â”€ ì œì£¼í•­ê³µ_í™˜ë¶ˆê·œì •.md
                â”œâ”€â”€ ì§„ì—ì–´_í™˜ë¶ˆê·œì •.md
                â”œâ”€â”€ ëŒ€í•œí•­ê³µ_í™˜ë¶ˆê·œì •.md
                â”œâ”€â”€ ì•„ì‹œì•„ë‚˜_í™˜ë¶ˆê·œì •.md
                â”œâ”€â”€ ì´ìŠ¤íƒ€í•­ê³µ_í™˜ë¶ˆê·œì •.md
                â””â”€â”€ ì—ì–´ì„œìš¸_í™˜ë¶ˆê·œì •.md
        ```
        """)
        st.stop()


    # MD íŒŒì¼ ë¡œë“œ
    docs = []
    # MD íŒŒì¼ ë¡œë“œ
    for md_file in md_files:
        try:
            loader = TextLoader(str(md_file), encoding='utf-8')
            file_docs = loader.load()

            for doc in file_docs:
                doc.metadata['source'] = md_file.name

            docs.extend(file_docs)
        except Exception as e:
            st.warning(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {md_file.name}")
            continue

    if not docs:
        st.error("MD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        st.stop()

    # After ì„¤ì •ìœ¼ë¡œ ì²­í‚¹ (í° ì²­í¬ë¡œ í‘œ ì™„ì „ í¬í•¨)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,  # After: í° ì²­í¬ (í‘œ ì˜ë¦¼ ë°©ì§€)
        chunk_overlap=400  # ì˜¤ë²„ë© ì¦ê°€
    )
    chunks = splitter.split_documents(docs)

    # ë²¡í„° DB ìƒì„±
    embeddings = OpenAIEmbeddings(
        model='text-embedding-3-small',
        api_key=os.environ.get("OPENAI_API_KEY")
    )

    db = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name="after_chatbot"
    )

    return db, len(docs), len(chunks)

with st.spinner("ğŸ”„ ë²¡í„° DB ì´ˆê¸°í™” ì¤‘..."):
    db, num_docs, num_chunks = initialize_vectordb()

st.sidebar.success(f"âœ… {num_docs}ê°œ ë¬¸ì„œ, {num_chunks}ê°œ ì²­í¬")

# ë¼ìš°íŒ… í‚¤ì›Œë“œ (After ë²„ì „ - ëŒ€í­ í™•ì¥)
RAG_KEYWORDS = [
    # === í™˜ë¶ˆ/ì·¨ì†Œ ê´€ë ¨ ===
    "í™˜ë¶ˆ", "ë¶ˆí™˜", "ë°˜í™˜", "ëŒë ¤", "ëŒë ¤ë°›", "ë¦¬í€", "refund",
    "ì·¨ì†Œ", "ìº”ìŠ¬", "cancel", "cancellation", "í•´ì§€", "ì² íšŒ",
    "ë¶€ë¶„í™˜ë¶ˆ", "ì „ì•¡í™˜ë¶ˆ", "ì¼ë¶€í™˜ë¶ˆ",

    # === ë³€ê²½ ê´€ë ¨ ===
    "ë³€ê²½", "ìˆ˜ì •", "êµí™˜", "ë°”ê¾¸", "ë°”ê¿”", "change", "modify", "modification",
    "ì¼ì •ë³€ê²½", "ë‚ ì§œë³€ê²½", "ì‹œê°„ë³€ê²½", "í¸ëª…ë³€ê²½", "ê²½ë¡œë³€ê²½",
    "ì¬ë°œê¶Œ", "ë¦¬ì´ìŠˆ", "reissue",

    # === ìˆ˜ìˆ˜ë£Œ ê´€ë ¨ ===
    "ìˆ˜ìˆ˜ë£Œ", "fee", "charge", "ë¹„ìš©", "ìš”ê¸ˆ", "ê°€ê²©", "ê¸ˆì•¡",
    "ìœ„ì•½ê¸ˆ", "íŒ¨ë„í‹°", "penalty", "ë²Œê¸ˆ",
    "ë³€ê²½ìˆ˜ìˆ˜ë£Œ", "í™˜ë¶ˆìˆ˜ìˆ˜ë£Œ", "ì·¨ì†Œìˆ˜ìˆ˜ë£Œ", "ì¬ë°œê¶Œìˆ˜ìˆ˜ë£Œ",
    "ë¬´ë£Œ", "ê³µì§œ", "ê½ì§œ", "ê½ì", "ê½ì", "free", "ë©´ì œ",

    # === í•­ê³µê¶Œ/í‹°ì¼“ ê´€ë ¨ ===
    "í•­ê³µê¶Œ", "í‹°ì¼“", "ticket", "í‘œ", "ë¹„í–‰ê¸°í‘œ", "í•­ê³µ", "í•­ê³µí¸",
    "í¸ëª…", "ì¢Œì„", "seat", "booking", "ì˜ˆì•½",

    # === ìš´ì„ ë“±ê¸‰ ===
    "ìš´ì„", "fare", "ë“±ê¸‰", "í´ë˜ìŠ¤", "class",

    # ê¸°ë³¸ ìš´ì„ ë“±ê¸‰ (ì œì£¼í•­ê³µ, ëŒ€í•œí•­ê³µ ë“±)
    "flex", "flexible", "í”Œë ‰ìŠ¤", "í”Œë ‰ì‹œë¸”",
    "standard", "ìŠ¤íƒ ë‹¤ë“œ",
    "saver", "ì„¸ì´ë²„", "save",
    "basic", "ë² ì´ì§", "ë² ì´ì‹",

    # ì´ìŠ¤íƒ€í•­ê³µ/ì•„ì‹œì•„ë‚˜ ìš´ì„ ì¢…ë¥˜
    "íŠ¹ê°€", "íŠ¹ê°€ìš´ì„", "í”„ë¡œëª¨ì…˜", "promotion", "special",
    "í• ì¸", "í• ì¸ìš´ì„", "discount", "ì„¸ì¼",
    "ì¼ë°˜", "ì¼ë°˜ìš´ì„", "ì •ìƒ", "ì •ìƒìš´ì„", "regular", "normal",

    # ì¢Œì„ ë“±ê¸‰
    "premium", "í”„ë¦¬ë¯¸ì—„", "ë¹„ì¦ˆ", "biz", "business","ë¹„ì¦ˆë‹ˆìŠ¤",
    "biz lite", "ë¹„ì¦ˆë¼ì´íŠ¸", "flex", "í”Œë ‰ìŠ¤","í”Œë™ìŠ¤", "standard", "ìŠ¤íƒ ë‹¤ë“œ","ìŠ¤í…ë‹¤ë“œ",
    "saver", "ì„¸ì´ë²„","ìƒˆì´ë²„", "basic", "ë² ì´ì‹","ë² ì´ì§","ë°°ì´ì‹","ë°°ì´ì§",
    "ì´ì½”ë…¸ë¯¸", "economy", "ì¼ë°˜ì„", "ë¹„ì¦ˆë‹ˆìŠ¤ì„", "ë¹„ì¦ˆë‹ˆìŠ¤","ì¼ë“±ì„", "í¼ìŠ¤íŠ¸","first",
    "í”„ë¦¬ë¯¸ì—„ ì´ì½”ë…¸ë¯¸", "í”„ë ˆìŠ¤í‹°ì§€","prestige",
    "W","T","L","S","V","K","H","E","Q","M","B","Y","Z","U","D","C","J",
    "ì§€ë‹ˆí”ŒëŸ¬ìŠ¤","ì§€ë‹ˆ","í”Œë ‰ìŠ¤","í”Œë™ìŠ¤","ìŠˆí¼ë¡œìš°","ì§€ë‹ˆë¹„ì¦ˆ"

    # === ë…¸ì„  êµ¬ë¶„ ===
    "êµ­ë‚´ì„ ", "êµ­ë‚´", "domestic", "ë„ë©”ìŠ¤í‹±",
    "êµ­ì œì„ ", "êµ­ì œ", "international", "ì¸í„°ë‚´ì…”ë„", "í•´ì™¸", "ì™¸êµ­"
    "ë‹¨ê±°ë¦¬", "ì¤‘ê±°ë¦¬", "ì¥ê±°ë¦¬", "short", "medium", "long",

    # === ë…¸ì‡¼ ê´€ë ¨ ===
    "ë…¸ì‡¼", "no-show", "noshow", "ë¯¸íƒ‘ìŠ¹", "ë¯¸ìŠ¹ì„ ", "ë¶ˆíƒ‘ìŠ¹",
    "ë¯¸ì·¨ì†Œ", "ë¯¸ì¶œí˜„", "ë¶ˆì¶œì„", "ì˜ˆì•½ë¶€ë„",
    "ê²Œì´íŠ¸", "gate", "ì¶œêµ¬ì¥", "íƒ‘ìŠ¹êµ¬",
    "íƒ‘ìŠ¹ìˆ˜ì†", "ì²´í¬ì¸", "check-in", "ìˆ˜ì†",

    # === ê¸°ê°„/ì‹œê°„ ê´€ë ¨ ===
    "ê¸°ê°„", "ê¸°í•œ", "ìœ íš¨", "ìœ íš¨ê¸°ê°„", "validity", "ë§Œë£Œ",
    "ì¶œë°œ", "ì¶œë°œì¼", "ì¶œë°œì „", "ì¶œë°œí›„", "departure",
    "ë‹¹ì¼", "ì˜¤ëŠ˜", "ë©°ì¹ ", "ëª‡ì¼", "ë©°ì¹ ì „", "ì¼ì „", "ì „",
    "ì´ì „", "ì´í›„", "before", "after",
    "91ì¼", "90ì¼", "60ì¼", "15ì¼", "14ì¼", "4ì¼", "3ì¼",

    # === ê·œì •/ì •ì±… ê´€ë ¨ ===
    "ê·œì •", "ì •ì±…", "policy", "ì•½ê´€", "ì¡°ê±´", "ê·œì¹™", "rule",
    "ê°€ëŠ¥", "ë¶ˆê°€", "ê°€ëŠ¥í•œ", "ì•ˆë˜", "ë˜ë‚˜", "í• ìˆ˜ìˆ", "í• ìˆ˜ì—†",

    # === í•­ê³µì‚¬ëª… ===
    "ëŒ€í•œí•­ê³µ", "ì•„ì‹œì•„ë‚˜", "ì œì£¼í•­ê³µ", "ì§„ì—ì–´", "í‹°ì›¨ì´","ì—ì–´ì„œìš¸","ì´ìŠ¤íƒ€í•­ê³µ",
    "korean", "koreanair", "asiana", "jeju", "jejuair", "jin", "jinair", "tway","airseoul","eastar"

    # === ì§ˆë¬¸ í‚¤ì›Œë“œ ===
    "ì–¸ì œ", "when", "ì–¼ë§ˆ", "how much", "ì–´ë””", "where",
    "ë¬´ì—‡", "what", "ì™œ", "why",
    "ê°€ëŠ¥í•´", "ë˜ë‚˜ìš”", "ì¸ê°€ìš”", "í•œê°€ìš”", "ë‚˜ìš”"
]

def route_to_rag(q):
    """ë¼ìš°íŒ… íŒë‹¨"""
    ql = q.lower()
    return any(kw.lower() in ql for kw in RAG_KEYWORDS)

def refund_rag(question, k_val, use_syn):
    """RAG ê²€ìƒ‰ ë° ë‹µë³€ (ë™ì˜ì–´ í™•ì¥ ì˜µì…˜)"""

    # ë™ì˜ì–´ í™•ì¥
    search_query = expand_query_with_synonyms(question) if use_syn else question

    results = db.similarity_search_with_relevance_scores(search_query, k=k_val)

    if not results:
        return "âŒ ê´€ë ¨ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", []

    context = "\n\n".join([doc.page_content for doc, score in results])
    answer = rag_chain.invoke({'context': context, 'q': question})

    sources = []
    for doc, score in results:
        sources.append({
            'filename': doc.metadata.get('source', 'unknown'),
            'score': score,
            'content': doc.page_content[:150]
        })

    return answer, sources

# ì±„íŒ… UI
for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì…ë ¥ ì²˜ë¦¬
if st.session_state["question"]:
    question = st.session_state["question"]
    st.session_state["question"] = None
else:
    question = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

if question:
    # ì‚¬ìš©ì ë©”ì‹œì§€
    with st.chat_message("user"):
        st.markdown(question)
    st.session_state["messages"].append({"role": "user", "content": question})

    # ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€
    with st.chat_message("assistant"):
        if route_to_rag(question):
            with st.spinner("ğŸ” ê²€ìƒ‰ ì¤‘..."):
                answer, sources = refund_rag(question, k, use_synonyms)

            st.markdown(answer)

            # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
            if sources:
                st.success(f"âœ… {len(sources)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ")
            else:
                st.error("âŒ ê²€ìƒ‰ ì‹¤íŒ¨")

            if show_sources and sources:
                with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ"):
                    for i, src in enumerate(sources, 1):
                        st.markdown(f"**{i}. {src['filename']}** (ìœ ì‚¬ë„: {src['score']:.2f})")
                        st.caption(src['content'][:100] + "...")
                        st.divider()
        else:
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤. í•­ê³µê¶Œ í™˜ë¶ˆ/ë³€ê²½ ê´€ë ¨ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
            st.markdown(answer)

    st.session_state["messages"].append({"role": "assistant", "content": answer})

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
st.caption("ğŸŸ¢ After ë²„ì „ - ìµœì¢… ê°œì„ ")
st.caption("íŠ¹ì§•: chunk_size=2000, ë™ì˜ì–´ 50+, í”„ë¡¬í”„íŠ¸ ê°œì„ , ëŒ€í•œí•­ê³µ í†µí•©")
