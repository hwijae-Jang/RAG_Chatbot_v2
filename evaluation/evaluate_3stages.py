
"""
GPT ì„±ëŠ¥ë¹„êµ ê°€ì¤‘ì¹˜ ì¡°ì ˆ
Before â†’ Middle â†’ After 3ë‹¨ê³„ ë¹„êµ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- 15ê°œ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ìœ¼ë¡œ ì²´ê³„ì  ë¹„êµ
- ê° ë‹¨ê³„ë³„ ê°œì„  íš¨ê³¼ ì •ëŸ‰í™”
"""

import os
import sys
import time
import json
from pathlib import Path
from typing import List, Dict
from datetime import datetime

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
TEST_DATASET = [
    {
        "id": 1,
        "question": "ì œì£¼í•­ê³µ êµ­ì œì„  FLEX ìš´ì„ì€ ì¶œë°œ ì „ í™˜ë¶ˆì´ ê°€ëŠ¥í•œê°€ìš”?",
        "keywords": ["FLEX", "ë¬´ë£Œ", "í™˜ë¶ˆ", "ê°€ëŠ¥"],
        "airline": "ì œì£¼í•­ê³µ",
        "category": "í™˜ë¶ˆ_ê°€ëŠ¥ì—¬ë¶€"
    },
    {
        "id": 2,
        "question": "ì§„ì—ì–´ êµ­ë‚´ì„  ë…¸ì‡¼ ìœ„ì•½ê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "keywords": ["30,000", "í¸ë„", "êµ­ë‚´ì„ "],
        "airline": "ì§„ì—ì–´",
        "category": "ë…¸ì‡¼_ìœ„ì•½ê¸ˆ"
    },
    {
        "id": 3,
        "question": "ëŒ€í•œí•­ê³µ êµ­ì œì„  ì¶œë°œ 5ì¼ ì „ ë³€ê²½í•˜ë©´ ìˆ˜ìˆ˜ë£Œê°€ ì–¼ë§ˆì¸ê°€ìš”?",
        "keywords": ["ìˆ˜ìˆ˜ë£Œ", "ìš´ì„", "ë“±ê¸‰"],
        "airline": "ëŒ€í•œí•­ê³µ",
        "category": "ë³€ê²½_ìˆ˜ìˆ˜ë£Œ"
    },
    {
        "id": 4,
        "question": "ì œì£¼í•­ê³µ BASIC ìš´ì„ê³¼ FLEX ìš´ì„ì˜ í™˜ë¶ˆ ê·œì • ì°¨ì´ëŠ”?",
        "keywords": ["BASIC", "ë¶ˆê°€", "FLEX", "ë¬´ë£Œ"],
        "airline": "ì œì£¼í•­ê³µ",
        "category": "ìš´ì„_ë¹„êµ"
    },
    {
        "id": 5,
        "question": "ì•„ì‹œì•„ë‚˜ íŠ¹ê°€ ìš´ì„ì€ í™˜ë¶ˆì´ ë˜ë‚˜ìš”?",
        "keywords": ["íŠ¹ê°€", "í™˜ë¶ˆ"],
        "airline": "ì•„ì‹œì•„ë‚˜",
        "category": "í™˜ë¶ˆ_ê°€ëŠ¥ì—¬ë¶€"
    },
    {
        "id": 6,
        "question": "í‹°ì›¨ì´ êµ­ì œì„  ì¶œë°œ 3ì¼ ì „ ì·¨ì†Œ ì‹œ ìˆ˜ìˆ˜ë£ŒëŠ”?",
        "keywords": ["ìˆ˜ìˆ˜ë£Œ", "3ì¼", "ì·¨ì†Œ"],
        "airline": "í‹°ì›¨ì´",
        "category": "í™˜ë¶ˆ_ìˆ˜ìˆ˜ë£Œ"
    },
    {
        "id": 7,
        "question": "ì—ì–´ì„œìš¸ ì²´í¬ì¸ í›„ íƒ‘ìŠ¹í•˜ì§€ ì•Šìœ¼ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "keywords": ["ë…¸ì‡¼", "ìœ„ì•½ê¸ˆ", "íƒ‘ìŠ¹"],
        "airline": "ì—ì–´ì„œìš¸",
        "category": "ë…¸ì‡¼_ìœ„ì•½ê¸ˆ"
    },
    {
        "id": 8,
        "question": "ì´ìŠ¤íƒ€í•­ê³µ ì¼ë°˜ìš´ì„ êµ­ë‚´ì„  í™˜ë¶ˆ ìˆ˜ìˆ˜ë£ŒëŠ”?",
        "keywords": ["ì¼ë°˜", "í™˜ë¶ˆ", "ìˆ˜ìˆ˜ë£Œ"],
        "airline": "ì´ìŠ¤íƒ€í•­ê³µ",
        "category": "í™˜ë¶ˆ_ìˆ˜ìˆ˜ë£Œ"
    },
    {
        "id": 9,
        "question": "ì§„ì—ì–´ì™€ ì œì£¼í•­ê³µì˜ ë…¸ì‡¼ ìœ„ì•½ê¸ˆì„ ë¹„êµí•´ì£¼ì„¸ìš”.",
        "keywords": ["ì§„ì—ì–´", "ì œì£¼í•­ê³µ", "ë…¸ì‡¼"],
        "airline": "ë³µìˆ˜",
        "category": "í•­ê³µì‚¬_ë¹„êµ"
    },
    {
        "id": 10,
        "question": "ëŒ€í•œí•­ê³µ í”„ë¦¬ë¯¸ì—„ ì´ì½”ë…¸ë¯¸ í™˜ë¶ˆ ê·œì •ì€?",
        "keywords": ["í™˜ë¶ˆ", "ìš´ì„"],
        "airline": "ëŒ€í•œí•­ê³µ",
        "category": "í™˜ë¶ˆ_ê°€ëŠ¥ì—¬ë¶€"
    },
    {
        "id": 11,
        "question": "ì œì£¼í•­ê³µ êµ­ì œì„  STANDARD ìš´ì„ ì¶œë°œ 15ì¼ ì „ í™˜ë¶ˆ ìˆ˜ìˆ˜ë£ŒëŠ”?",
        "keywords": ["STANDARD", "ìˆ˜ìˆ˜ë£Œ"],
        "airline": "ì œì£¼í•­ê³µ",
        "category": "í™˜ë¶ˆ_ìˆ˜ìˆ˜ë£Œ"
    },
    {
        "id": 12,
        "question": "ë…¸ì‡¼ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "keywords": ["ë…¸ì‡¼", "íƒ‘ìŠ¹", "ë¯¸íƒ‘ìŠ¹"],
        "airline": "ì¼ë°˜",
        "category": "ìš©ì–´_ì„¤ëª…"
    },
    {
        "id": 13,
        "question": "ì§„ì—ì–´ êµ­ì œì„  ë…¸ì‡¼ ìœ„ì•½ê¸ˆì€ ì™•ë³µ ê¸°ì¤€ ì–¼ë§ˆì¸ê°€ìš”?",
        "keywords": ["100,000", "ì™•ë³µ", "êµ­ì œì„ "],
        "airline": "ì§„ì—ì–´",
        "category": "ë…¸ì‡¼_ìœ„ì•½ê¸ˆ"
    },
    {
        "id": 14,
        "question": "í•­ê³µê¶Œì„ í™˜ë¶ˆë°›ìœ¼ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
        "keywords": ["í™˜ë¶ˆ", "ì‹ ì²­"],
        "airline": "ì¼ë°˜",
        "category": "ì ˆì°¨_ì•ˆë‚´"
    },
    {
        "id": 15,
        "question": "ëŒ€í•œí•­ê³µ êµ­ë‚´ì„ ê³¼ êµ­ì œì„ ì˜ í™˜ë¶ˆ ê·œì • ì°¨ì´ëŠ”?",
        "keywords": ["êµ­ë‚´ì„ ", "êµ­ì œì„ ", "ì°¨ì´"],
        "airline": "ëŒ€í•œí•­ê³µ",
        "category": "ë…¸ì„ _ë¹„êµ"
    }
]


class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ í‰ê°€ê¸°"""

    def __init__(self, version: str, md_path: str, chunk_size: int, chunk_overlap: int,
                 synonyms: dict = None):
        self.version = version
        self.md_path = md_path
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.synonyms = synonyms or {}

        # API í‚¤ í™•ì¸
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

        # LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model='gpt-4o-mini',
            temperature=0,
            api_key=self.api_key
        )

        # í”„ë¡¬í”„íŠ¸
        self.rag_chain = (
            ChatPromptTemplate.from_template("""
ë„ˆëŠ” í•­ê³µê¶Œ í™˜ë¶ˆ ë° ë³€ê²½ì„ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•œ í•œêµ­ì–´ ìƒë‹´ ì±—ë´‡ì´ì•¼.
ì•„ë˜ í•­ê³µì‚¬ ì •ì±… ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜.

ì •ì±…ë¬¸ì„œ: {context}
ì‚¬ìš©ì ì§ˆë¬¸: {q}

ë‹µë³€:
""") | self.llm | StrOutputParser()
        )

        self.db = None

    def initialize(self):
        """ë²¡í„° DB ì´ˆê¸°í™”"""
        print(f"\nğŸ”§ {self.version} ë²„ì „ ì´ˆê¸°í™”...")
        print(f"   Chunk Size: {self.chunk_size}")
        print(f"   Chunk Overlap: {self.chunk_overlap}")
        print(f"   Synonyms: {len(self.synonyms)}ê°œ")

        # MD íŒŒì¼ ë¡œë“œ
        docs = []
        md_dir = Path(self.md_path)

        for md_file in md_dir.glob("*.md"):
            try:
                loader = TextLoader(str(md_file), encoding='utf-8')
                file_docs = loader.load()

                for doc in file_docs:
                    doc.metadata['source'] = md_file.name

                docs.extend(file_docs)
            except Exception as e:
                print(f"   âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {md_file.name}")
                continue

        print(f"   ğŸ“ {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œ")

        # ì²­í‚¹
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        chunks = splitter.split_documents(docs)
        print(f"   ğŸ“„ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        # ë²¡í„° DB
        embeddings = OpenAIEmbeddings(
            model='text-embedding-3-small',
            api_key=self.api_key
        )

        self.db = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            collection_name=f"{self.version}_eval"
        )

        print(f"   âœ… ì´ˆê¸°í™” ì™„ë£Œ")

    def expand_query(self, query: str) -> str:
        """ë™ì˜ì–´ë¡œ ì¿¼ë¦¬ í™•ì¥"""
        if not self.synonyms:
            return query

        words = query.split()
        expanded = []

        for word in words:
            word_lower = word.lower()
            if word_lower in self.synonyms:
                expanded.extend(self.synonyms[word_lower][:2])
            expanded.append(word)

        return " ".join(expanded)

    def evaluate_answer_accuracy_with_llm(self, question: str, answer: str, ground_truth_context: str) -> Dict:
        """
        LLM-as-Judge: GPTë¡œ ë‹µë³€ ì •í™•ë„ í‰ê°€

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            answer: RAG ì‹œìŠ¤í…œì˜ ë‹µë³€
            ground_truth_context: MD íŒŒì¼ì—ì„œ ê²€ìƒ‰ëœ ì‹¤ì œ ë‚´ìš© (ì •ë‹µ ê·¼ê±°)

        Returns:
            {
                "accuracy_score": float (0-1),
                "factual_correctness": float (0-1),
                "completeness": float (0-1),
                "evaluation_reason": str
            }
        """

        evaluation_prompt = f"""ë‹¹ì‹ ì€ í•­ê³µê¶Œ í™˜ë¶ˆ ê·œì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•œ ì±—ë´‡ ë‹µë³€ì´ ì •ë‹µ ê·¼ê±° ìë£Œì™€ ë¹„êµí–ˆì„ ë•Œ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

**ì§ˆë¬¸**: {question}

**ì •ë‹µ ê·¼ê±° (MD íŒŒì¼ ë‚´ìš©)**:
{ground_truth_context}

**ì±—ë´‡ ë‹µë³€**:
{answer}

ë‹¤ìŒ 2ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ 0-100ì  ì²™ë„ë¡œ í‰ê°€í•˜ê³ , JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

1. **ì‚¬ì‹¤ ì •í™•ì„± (factual_correctness)**: ì±—ë´‡ ë‹µë³€ì´ ì •ë‹µ ê·¼ê±°ì™€ ì‚¬ì‹¤ì ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ”ê°€? (0-100)
2. **ì™„ê²°ì„± (completeness)**: ì§ˆë¬¸ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶©ë¶„íˆ í¬í•¨í•˜ê³  ìˆëŠ”ê°€? (0-100)

**ì¤‘ìš”**:
- ì±—ë´‡ ë‹µë³€ì´ ì •ë‹µ ê·¼ê±°ì— ì—†ëŠ” ë‚´ìš©ì„ ì§€ì–´ëƒˆë‹¤ë©´ factual_correctnessë¥¼ 0ì ìœ¼ë¡œ í‰ê°€
- ì •ë‹µ ê·¼ê±°ì— ìˆëŠ” í•µì‹¬ ì •ë³´ë¥¼ ëˆ„ë½í–ˆë‹¤ë©´ completenessë¥¼ ë‚®ê²Œ í‰ê°€
- í‘œí˜„ì´ ë‹¤ë¥´ë”ë¼ë„ ì˜ë¯¸ê°€ ê°™ìœ¼ë©´ ë†’ì€ ì ìˆ˜ ë¶€ì—¬

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš” (ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):
{{
    "factual_correctness": 85,
    "completeness": 90,
    "evaluation_reason": "í‰ê°€ ì´ìœ ë¥¼ 1-2ë¬¸ì¥ìœ¼ë¡œ"
}}
"""

        try:
            # LLMìœ¼ë¡œ í‰ê°€
            eval_llm = ChatOpenAI(
                model='gpt-4o-mini',
                temperature=0,
                api_key=self.api_key
            )

            eval_result = eval_llm.invoke(evaluation_prompt)

            # JSON íŒŒì‹±
            import json
            import re

            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            content = eval_result.content
            json_match = re.search(r'\{[^}]+\}', content, re.DOTALL)

            if json_match:
                eval_data = json.loads(json_match.group())

                # GPT ì‘ë‹µì—ì„œ ì ìˆ˜ ì¶”ì¶œ
                factual = eval_data.get("factual_correctness", 0) / 100
                complete = eval_data.get("completeness", 0) / 100

                # ========================================
                # ğŸ¯ ê°€ì¤‘ì¹˜ ì¡°ì •
                # ========================================
                # ì˜µì…˜ 1: ì‚¬ì‹¤ ì¤‘ì‹œ (80% + 20%)
                accuracy = (factual * 0.8) + (complete * 0.2)

                # ì˜µì…˜ 2: ê· í˜• (60% + 40%) - ê¸°ë³¸ê°’
                # accuracy = (factual * 0.6) + (complete * 0.4)

                # ì˜µì…˜ 3: ì™„ê²°ì„± ì¤‘ì‹œ (40% + 60%)
                # accuracy = (factual * 0.4) + (complete * 0.6)
                # ========================================

                return {
                    "accuracy_score": accuracy,  # ì¬ê³„ì‚°ëœ ì¢…í•© ì ìˆ˜
                    "factual_correctness": factual,
                    "completeness": complete,
                    "evaluation_reason": eval_data.get("evaluation_reason", "í‰ê°€ ì‹¤íŒ¨")
                }
            else:
                return {
                    "accuracy_score": 0.0,
                    "factual_correctness": 0.0,
                    "completeness": 0.0,
                    "evaluation_reason": "JSON íŒŒì‹± ì‹¤íŒ¨"
                }

        except Exception as e:
            print(f"   âš ï¸ LLM í‰ê°€ ì‹¤íŒ¨: {e}")
            return {
                "accuracy_score": 0.0,
                "factual_correctness": 0.0,
                "completeness": 0.0,
                "evaluation_reason": f"ì˜¤ë¥˜: {str(e)}"
            }

    def evaluate_single(self, test: Dict) -> Dict:
        """ë‹¨ì¼ ì§ˆë¬¸ í‰ê°€"""
        question = test["question"]
        keywords = test["keywords"]

        # ê²€ìƒ‰
        start_time = time.time()
        search_query = self.expand_query(question)
        results = self.db.similarity_search_with_relevance_scores(search_query, k=3)
        search_time = time.time() - start_time

        # ë‹µë³€ ìƒì„±
        if results:
            context = "\n\n".join([doc.page_content for doc, _ in results])
            gen_start = time.time()
            answer = self.rag_chain.invoke({'context': context, 'q': question})
            gen_time = time.time() - gen_start
        else:
            context = ""
            answer = "ê´€ë ¨ ê·œì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            gen_time = 0

        total_time = time.time() - start_time

        # í‚¤ì›Œë“œ ë§¤ì¹­ (ê¸°ì¡´ ë°©ì‹)
        answer_lower = answer.lower()
        keywords_found = [kw for kw in keywords if kw.lower() in answer_lower]
        keyword_score = len(keywords_found) / len(keywords) if keywords else 0

        # LLM-as-Judge í‰ê°€ (ìƒˆë¡œìš´ ë°©ì‹)
        if context:
            llm_evaluation = self.evaluate_answer_accuracy_with_llm(question, answer, context)
        else:
            llm_evaluation = {
                "accuracy_score": 0.0,
                "factual_correctness": 0.0,
                "completeness": 0.0,
                "evaluation_reason": "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
            }

        return {
            "id": test["id"],
            "question": question,
            "answer": answer,
            "keywords_expected": keywords,
            "keywords_found": keywords_found,
            "keyword_score": keyword_score,
            "search_success": len(results) > 0,
            "num_sources": len(results),
            "search_time": search_time,
            "generation_time": gen_time,
            "total_time": total_time,
            "category": test["category"],
            # LLM í‰ê°€ ì¶”ê°€
            "llm_accuracy_score": llm_evaluation["accuracy_score"],
            "llm_factual_correctness": llm_evaluation["factual_correctness"],
            "llm_completeness": llm_evaluation["completeness"],
            "llm_evaluation_reason": llm_evaluation["evaluation_reason"]
        }

    def evaluate_all(self, dataset: List[Dict]) -> List[Dict]:
        """ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€"""
        print(f"\nğŸ“Š {self.version} í‰ê°€ ì‹œì‘ ({len(dataset)}ê°œ ì§ˆë¬¸)")

        results = []
        for i, test in enumerate(dataset, 1):
            print(f"[{i}/{len(dataset)}] {test['question'][:40]}...")

            result = self.evaluate_single(test)

            print(f"   í‚¤ì›Œë“œ: {result['keyword_score']:.0%} | LLMí‰ê°€: {result['llm_accuracy_score']:.0%} | ê²€ìƒ‰: {'âœ…' if result['search_success'] else 'âŒ'} | ì‹œê°„: {result['total_time']:.2f}s")
            if result['llm_evaluation_reason']:
                print(f"   ğŸ“ {result['llm_evaluation_reason']}")

            results.append(result)

        return results


def calculate_metrics(results: List[Dict]) -> Dict:
    """ì¢…í•© ì§€í‘œ ê³„ì‚°"""

    keyword_scores = [r['keyword_score'] for r in results]
    avg_keyword = sum(keyword_scores) / len(keyword_scores)

    # LLM í‰ê°€ ì ìˆ˜ ì¶”ê°€
    llm_accuracy_scores = [r.get('llm_accuracy_score', 0) for r in results]
    llm_factual_scores = [r.get('llm_factual_correctness', 0) for r in results]
    llm_completeness_scores = [r.get('llm_completeness', 0) for r in results]

    avg_llm_accuracy = sum(llm_accuracy_scores) / len(llm_accuracy_scores)
    avg_llm_factual = sum(llm_factual_scores) / len(llm_factual_scores)
    avg_llm_completeness = sum(llm_completeness_scores) / len(llm_completeness_scores)

    search_successes = [1 for r in results if r['search_success']]
    search_rate = len(search_successes) / len(results)

    times = [r['total_time'] for r in results]
    avg_time = sum(times) / len(times)

    categories = {}
    for r in results:
        cat = r['category']
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(r['keyword_score'])

    category_scores = {
        cat: sum(scores) / len(scores)
        for cat, scores in categories.items()
    }

    return {
        "avg_keyword_score": avg_keyword,
        "avg_llm_accuracy": avg_llm_accuracy,
        "avg_llm_factual": avg_llm_factual,
        "avg_llm_completeness": avg_llm_completeness,
        "search_success_rate": search_rate,
        "avg_response_time": avg_time,
        "category_scores": category_scores
    }


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*70)
    print("ğŸš€ Before â†’ Middle â†’ After 3ë‹¨ê³„ ë¹„êµ í‰ê°€")
    print("="*70)

    # Before ë™ì˜ì–´ ì‚¬ì „ (ì—†ìŒ)
    BEFORE_SYNONYMS = {}

    # Middle ë™ì˜ì–´ ì‚¬ì „ (30ê°œ)
    MIDDLE_SYNONYMS = {
        "ë…¸ì‡¼": ["ë…¸ì‡¼", "no-show", "ì˜ˆì•½ë¶€ë„"],
        "no-show": ["ë…¸ì‡¼", "no-show", "ì˜ˆì•½ë¶€ë„"],
        "ì˜ˆì•½ë¶€ë„": ["ë…¸ì‡¼", "ì˜ˆì•½ë¶€ë„"],
        "í™˜ë¶ˆ": ["í™˜ë¶ˆ", "refund"],
        "refund": ["í™˜ë¶ˆ", "refund"],
        "ìˆ˜ìˆ˜ë£Œ": ["ìˆ˜ìˆ˜ë£Œ", "fee", "penalty"],
        "fee": ["ìˆ˜ìˆ˜ë£Œ", "fee"],
        "ìœ„ì•½ê¸ˆ": ["ìœ„ì•½ê¸ˆ", "penalty"],
        "flex": ["í”Œë ‰ìŠ¤", "FLEX"],
        "basic": ["ë² ì´ì§", "BASIC"],
    }

    # After ë™ì˜ì–´ ì‚¬ì „ (50+)
    AFTER_SYNONYMS = {
        # ë…¸ì‡¼ ê´€ë ¨
    "ë…¸ì‡¼": ["ë…¸ì‡¼", "No-Show", "no-show", "ë…¸ ì‡¼", "ë¯¸íƒ‘ìŠ¹", "ì˜ˆì•½ë¶€ë„"],
    "no-show": ["ë…¸ì‡¼", "No-Show", "no-show", "ë¯¸íƒ‘ìŠ¹", "ì˜ˆì•½ë¶€ë„"],
    "ì˜ˆì•½ë¶€ë„": ["ë…¸ì‡¼", "ì˜ˆì•½ë¶€ë„", "no-show", "ë¯¸íƒ‘ìŠ¹"],

    # í™˜ë¶ˆ ê´€ë ¨
    "í™˜ë¶ˆ": ["í™˜ë¶ˆ", "refund", "ë°˜í™˜", "ì·¨ì†Œí™˜ë¶ˆ"],
    "refund": ["í™˜ë¶ˆ", "refund", "ë°˜í™˜"],

    # ë³€ê²½ ê´€ë ¨
    "ë³€ê²½": ["ë³€ê²½", "change", "ìˆ˜ì •", "êµí™˜"],
    "change": ["ë³€ê²½", "change", "ìˆ˜ì •"],

    # ìˆ˜ìˆ˜ë£Œ ê´€ë ¨
    "ìˆ˜ìˆ˜ë£Œ": ["ìˆ˜ìˆ˜ë£Œ", "fee", "ìš”ê¸ˆ", "ë¹„ìš©", "charge", "ìœ„ì•½ê¸ˆ", "íŒ¨ë„í‹°", "penalty"],
    "fee": ["ìˆ˜ìˆ˜ë£Œ", "fee", "ìš”ê¸ˆ", "ë¹„ìš©", "charge", "ìœ„ì•½ê¸ˆ", "íŒ¨ë„í‹°", "penalty"],
    "ìœ„ì•½ê¸ˆ": ["ìœ„ì•½ê¸ˆ", "íŒ¨ë„í‹°", "penalty", "ìˆ˜ìˆ˜ë£Œ", "fee"],
    "penalty": ["ìœ„ì•½ê¸ˆ", "íŒ¨ë„í‹°", "penalty", "ìˆ˜ìˆ˜ë£Œ"],

    # ì·¨ì†Œ ê´€ë ¨
    "ì·¨ì†Œ": ["ì·¨ì†Œ", "cancel", "cancellation", "í•´ì§€"],
    "cancel": ["ì·¨ì†Œ", "cancel", "cancellation"],

    # ìš´ì„ ì¢…ë¥˜
    "íŠ¹ê°€": ["íŠ¹ê°€", "íŠ¹ê°€ìš´ì„", "í”„ë¡œëª¨ì…˜", "promotion", "special"],
    "í• ì¸": ["í• ì¸", "í• ì¸ìš´ì„", "discount", "ì„¸ì¼", "sale"],
    "ì¼ë°˜": ["ì¼ë°˜", "ì¼ë°˜ìš´ì„", "ì •ìƒ", "ì •ìƒìš´ì„", "normal", "regular"],

    # ìš´ì„ ë“±ê¸‰
    "ë² ì´ì§": ["ë² ì´ì§", "BASIC", "Basic", "basic"],
    "basic": ["ë² ì´ì§", "BASIC", "Basic"],
    "ìŠ¤íƒ ë‹¤ë“œ": ["ìŠ¤íƒ ë‹¤ë“œ", "STANDARD", "Standard", "standard"],
    "standard": ["ìŠ¤íƒ ë‹¤ë“œ", "STANDARD", "Standard"],
    "í”Œë ‰ìŠ¤": ["í”Œë ‰ìŠ¤", "FLEX", "Flex", "flex", "flexible"],
    "flex": ["í”Œë ‰ìŠ¤", "FLEX", "Flex", "flexible"],
    "ì„¸ì´ë²„": ["ì„¸ì´ë²„", "SAVER", "Saver", "saver"],
    "saver": ["ì„¸ì´ë²„", "SAVER", "Saver"],

    # ë…¸ì„  ê´€ë ¨
    "êµ­ë‚´ì„ ": ["êµ­ë‚´ì„ ", "domestic", "êµ­ë‚´"],
    "domestic": ["êµ­ë‚´ì„ ", "domestic"],
    "êµ­ì œì„ ": ["êµ­ì œì„ ", "international", "êµ­ì œ", "í•´ì™¸"],
    "international": ["êµ­ì œì„ ", "international"],

    # íƒ‘ìŠ¹ìˆ˜ì† ê´€ë ¨
    "íƒ‘ìŠ¹ìˆ˜ì†": ["íƒ‘ìŠ¹ìˆ˜ì†", "ì²´í¬ì¸", "check-in", "ìˆ˜ì†"],
    "ì²´í¬ì¸": ["íƒ‘ìŠ¹ìˆ˜ì†", "ì²´í¬ì¸", "check-in"],
    "check-in": ["íƒ‘ìŠ¹ìˆ˜ì†", "ì²´í¬ì¸", "check-in"],

    # ê²Œì´íŠ¸ ê´€ë ¨
    "ê²Œì´íŠ¸": ["ê²Œì´íŠ¸", "gate", "ì¶œêµ¬ì¥"],
    "gate": ["ê²Œì´íŠ¸", "gate"],

    # ë¯¸íƒ‘ìŠ¹
    "ë¯¸íƒ‘ìŠ¹": ["ë¯¸íƒ‘ìŠ¹", "no-show", "ë¯¸ìŠ¹ì„ ", "ë¶ˆíƒ‘ìŠ¹"]
    }

    # Before í‰ê°€
    before_eval = RAGEvaluator(
        version="Before",
        md_path="/content/before",
        chunk_size=800,
        chunk_overlap=100,
        synonyms=BEFORE_SYNONYMS
    )
    before_eval.initialize()
    before_results = before_eval.evaluate_all(TEST_DATASET)
    before_metrics = calculate_metrics(before_results)

    # Middle í‰ê°€
    middle_eval = RAGEvaluator(
        version="Middle",
        md_path="/content/middle",
        chunk_size=1200,
        chunk_overlap=200,
        synonyms=MIDDLE_SYNONYMS
    )
    middle_eval.initialize()
    middle_results = middle_eval.evaluate_all(TEST_DATASET)
    middle_metrics = calculate_metrics(middle_results)

    # After í‰ê°€
    after_eval = RAGEvaluator(
        version="After",
        md_path="/content/after",
        chunk_size=2000,
        chunk_overlap=400,
        synonyms=AFTER_SYNONYMS
    )
    after_eval.initialize()
    after_results = after_eval.evaluate_all(TEST_DATASET)
    after_metrics = calculate_metrics(after_results)

    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*70)
    print("ğŸ“Š 3ë‹¨ê³„ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("="*70)

    print(f"\n{'ì§€í‘œ':<25} {'Before':>15} {'Middle':>15} {'After':>15}")
    print("-"*70)

    # í‚¤ì›Œë“œ ì •í™•ë„
    print(f"{'í‚¤ì›Œë“œ ì •í™•ë„':<25} {before_metrics['avg_keyword_score']:>14.1%} {middle_metrics['avg_keyword_score']:>14.1%} {after_metrics['avg_keyword_score']:>14.1%}")

    # LLM í‰ê°€ ì ìˆ˜ ì¶”ê°€
    print(f"{'LLM ì¢…í•© ì •í™•ë„':<25} {before_metrics['avg_llm_accuracy']:>14.1%} {middle_metrics['avg_llm_accuracy']:>14.1%} {after_metrics['avg_llm_accuracy']:>14.1%}")
    print(f"{'LLM ì‚¬ì‹¤ ì •í™•ì„±':<25} {before_metrics['avg_llm_factual']:>14.1%} {middle_metrics['avg_llm_factual']:>14.1%} {after_metrics['avg_llm_factual']:>14.1%}")
    print(f"{'LLM ì™„ê²°ì„±':<25} {before_metrics['avg_llm_completeness']:>14.1%} {middle_metrics['avg_llm_completeness']:>14.1%} {after_metrics['avg_llm_completeness']:>14.1%}")

    # ê²€ìƒ‰ ì„±ê³µë¥ 
    print(f"{'ê²€ìƒ‰ ì„±ê³µë¥ ':<25} {before_metrics['search_success_rate']:>14.1%} {middle_metrics['search_success_rate']:>14.1%} {after_metrics['search_success_rate']:>14.1%}")

    # ì‘ë‹µ ì‹œê°„
    print(f"{'í‰ê·  ì‘ë‹µ ì‹œê°„':<25} {before_metrics['avg_response_time']:>13.2f}s {middle_metrics['avg_response_time']:>13.2f}s {after_metrics['avg_response_time']:>13.2f}s")

    # ê°œì„ ìœ¨ ê³„ì‚°
    print("\n" + "="*70)
    print("ğŸ“ˆ ë‹¨ê³„ë³„ ê°œì„ ìœ¨")
    print("="*70)

    kw_before_middle = (middle_metrics['avg_keyword_score'] - before_metrics['avg_keyword_score']) * 100
    kw_middle_after = (after_metrics['avg_keyword_score'] - middle_metrics['avg_keyword_score']) * 100
    kw_before_after = (after_metrics['avg_keyword_score'] - before_metrics['avg_keyword_score']) * 100

    print(f"\ní‚¤ì›Œë“œ ì •í™•ë„:")
    print(f"   Before â†’ Middle: +{kw_before_middle:.1f}%p")
    print(f"   Middle â†’ After:  +{kw_middle_after:.1f}%p")
    print(f"   Before â†’ After:  +{kw_before_after:.1f}%p (ì´ ê°œì„ )")

    # LLM í‰ê°€ ê°œì„ ìœ¨
    llm_before_middle = (middle_metrics['avg_llm_accuracy'] - before_metrics['avg_llm_accuracy']) * 100
    llm_middle_after = (after_metrics['avg_llm_accuracy'] - middle_metrics['avg_llm_accuracy']) * 100
    llm_before_after = (after_metrics['avg_llm_accuracy'] - before_metrics['avg_llm_accuracy']) * 100

    print(f"\nLLM ì¢…í•© ì •í™•ë„:")
    print(f"   Before â†’ Middle: +{llm_before_middle:.1f}%p")
    print(f"   Middle â†’ After:  +{llm_middle_after:.1f}%p")
    print(f"   Before â†’ After:  +{llm_before_after:.1f}%p (ì´ ê°œì„ )")

    sr_before_middle = (middle_metrics['search_success_rate'] - before_metrics['search_success_rate']) * 100
    sr_middle_after = (after_metrics['search_success_rate'] - middle_metrics['search_success_rate']) * 100
    sr_before_after = (after_metrics['search_success_rate'] - before_metrics['search_success_rate']) * 100

    print(f"\nê²€ìƒ‰ ì„±ê³µë¥ :")
    print(f"   Before â†’ Middle: +{sr_before_middle:.1f}%p")
    print(f"   Middle â†’ After:  +{sr_middle_after:.1f}%p")
    print(f"   Before â†’ After:  +{sr_before_after:.1f}%p (ì´ ê°œì„ )")

    # ê²°ê³¼ ì €ì¥
    output = {
        "timestamp": datetime.now().isoformat(),
        "before": {
            "results": before_results,
            "metrics": before_metrics
        },
        "middle": {
            "results": middle_results,
            "metrics": middle_metrics
        },
        "after": {
            "results": after_results,
            "metrics": after_metrics
        }
    }

    output_file = f"evaluation_3stages_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ ê²°ê³¼ ì €ì¥: {output_file}")

    return output


if __name__ == "__main__":
    main()

